# ‚òï CoffeeBot ‚Äî Chat Conversacional sobre Caf√©

CoffeeBot es un chatbot inteligente dise√±ado para responder exclusivamente preguntas sobre caf√©. Usa una arquitectura basada en RAG (Retrieval Augmented Generation) para responder de forma precisa, validando idioma y tem√°tica antes de mostrar la respuesta final.

Este proyecto fue desarrollado como take-home assignment y simula un producto listo para producci√≥n, con backend (FastAPI), frontend (Streamlit), tests, contenedores Docker, y CI/CD para despliegue en la nube.

---

## üß† Razonamiento detr√°s del dise√±o

CoffeeBot fue pensado bajo principios de **modularidad**, **validaci√≥n robusta**, y **desacoplamiento entre componentes**. Algunos puntos clave del enfoque:

- **Backend en FastAPI**: sirve como API para manejar preguntas, ejecutar RAG, llamar al modelo de lenguaje y validar las respuestas.
- **RAG local con FAISS**: para evitar dependencias externas, se realiza la indexaci√≥n y recuperaci√≥n en memoria a partir de un PDF (`coffee-recipes.pdf`).
- **Validaciones avanzadas**:
  - **Idioma**: solo se permite espa√±ol. Se detecta el input y tambi√©n el output del LLM para evitar respuestas en ingl√©s u otros idiomas.
  - **Tem√°tica**: si la respuesta no tiene suficiente similitud con vectores relacionados al caf√©, se descarta y se regenera autom√°ticamente.
- **LLM wrapper**: el dise√±o est√° desacoplado para permitir f√°cilmente cambiar el modelo base (Gemini, Mistral, TinyLlama, etc.).
- **Frontend en Streamlit**: visual profesional con burbujas de conversaci√≥n, scroll independiente y UX clara (feedback cuando se regenera una respuesta).
- **CI/CD**: preparado para Docker + DockerHub + Render o Kubernetes.
- **Conversaci√≥n con contexto**: se mantiene el historial de usuario y bot para mejorar la comprensi√≥n de cada mensaje.

---

## ‚ñ∂Ô∏è C√≥mo correr la app localmente

Requisitos:

- Docker instalado
- Python 3.10+ (solo si quer√©s correr sin Docker)

### üê≥ Opci√≥n A: usando Docker 

```bash
# Clon√° el repo
git clone https://github.com/Nemo-sop/CoffeeBot.git
cd CoffeeBot

# Build y correr backend y frontend
docker compose up --build
```
Esto expondr√°:
http://localhost:8000 ‚Üí API de FastAPI
http://localhost:8501 ‚Üí Interfaz en Streamlit

### üß™ Opci√≥n B: correr manualmente 

1. Backend
```bash
cd backend
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```
2. Frontend
En otro terminal:

```bash
cd frontend
pip install -r requirements.txt
streamlit run streamlit_app.py
```
---

## ‚öôÔ∏è Variables de entorno
Crear archivo .env con:

```bash
GEMINI_API_KEY=tu_clave_de_gemini
LOG_LEVEL=INFO
APP_ENV=development
```
---

## üß† Consideraciones
El PDF se carga e indexa al iniciar la API. Esto puede tomar unos segundos la primera vez.
La validaci√≥n de respuestas es estricta: si el bot responde en ingl√©s o con baja relevancia, se regenera autom√°ticamente la respuesta hasta un maximo de 3 veces.
No se usa una base vectorial externa: FAISS funciona en memoria para facilitar despliegue r√°pido.
No se permite hablar de temas no relacionados al caf√© ni en otro idioma que no sea el espa√±ol. El bot lo expresar√° expl√≠citamente.

---
## üöß TODOs / Mejoras futuras
 - Agregar soporte para respuestas enriquecidas (im√°genes, recetas, enlaces)
 - Persistir conversaciones y m√©tricas con una base de datos
 - Entrenar un modelo embebido especializado en caf√© como endgame
 - Tener un modelo en local como fallback cuando el free tier de gemini esta muy lento. ej: Mistral-7B
 - Deploy automatizado con GitHub Actions + DockerHub
 - M√°s validaciones sem√°nticas sobre respuestas (al menos ahcer un analisis mas profundo para settear un mejor threshold de similitud)
 - UI m√°s responsiva para mobile (ajustes de CSS) y en general mas atractiva para mejorar la UX
 - Panel administrativo para auditar conversaciones y rendimiento
 - Autenticaci√≥n para personalizar respuestas por usuario y un mejor manejod e sesiones
 
 ---
 
 ## üí° Modal como soluci√≥n t√©cnica viable (no implementada)
 
 Uno de los requerimientos del proyecto es el uso de FastAPI como framework para construir el backend. La aplicaci√≥n cumple con este requisito desde su dise√±o inicial: el backend completo est√° construido con FastAPI, incluyendo sus rutas, validaciones, esquema OpenAPI/Swagger y arquitectura modular.

Sin embargo, debido a las restricciones de memoria de servicios como Render Free Tier (512‚ÄØMiB), el backend no puede ser desplegado all√≠ de forma confiable, ya que realiza:

- Indexaci√≥n en memoria de un PDF con FAISS
- Generaci√≥n de embeddings con Gemini
- Procesamiento intensivo al iniciar


Durante el desarrollo del proyecto, se consider√≥ el uso de Modal como una soluci√≥n alternativa para desplegar el backend. Modal es una plataforma serverless que permite exponer aplicaciones web como funciones escalables con mayor capacidad de c√≥mputo que Render Free Tier (512‚ÄØMiB de RAM).

Sin embargo, no se implement√≥ esta opci√≥n por respeto a las pautas del assignment, que indicaban expl√≠citamente el uso de FastAPI. Aunque Modal permite montar apps FastAPI como aplicaciones ASGI, hacerlo requiere modificar el punto de entrada (uvicorn ‚Üí modal.asgi_app()), lo que puede ser interpretado como un desv√≠o del requisito original.

Aclaraci√≥n importante:

- Toda la l√≥gica del backend est√° desarrollada en FastAPI
- La arquitectura es completamente compatible con Modal

La opci√≥n de usar Modal fue analizada y documentada como una mejora factible para superar la limitaci√≥n de memoria sin romper el dise√±o base
 
 ---
 
## ‚è≥ Consideraci√≥n sobre el tiempo de desarrollo
Se hizo un esfuerzo consciente por respetar la estimaci√≥n de tiempo original del assignment, que indicaba que la tarea pod√≠a completarse en unas pocas horas.

Para cumplir con esta expectativa:

- Se prioriz√≥ una arquitectura funcional y escalable sin sobre-ingenier√≠a
- Se documentaron decisiones t√©cnicas sin extender el alcance con funcionalidades no requeridas
- Se dej√≥ expl√≠cito en la secci√≥n de TODOs qu√© mejoras fueron identificadas pero no implementadas

El objetivo fue entregar una soluci√≥n madura, pero alineada con los l√≠mites temporales propuestos, tal como se esperar√≠a en un entorno profesional.
